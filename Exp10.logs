Loading the imdb reviews data
Data loaded.
Extracting features from the training dataset using a sparse vectorizer
Feature extraction technique is CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=5,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None).
done in 8.536986351013184s
n_samples: 12500, n_features: 19271 

Loading took 10.36s.

i = 10	new_error = 0.40916 added
i = 17	new_error = 0.40742 added
i = 19	new_error = 0.38264 added
i = 30	new_error = 0.37593 added
i = 34	new_error = 0.36298 added
i = 81	new_error = 0.36173 added
i = 88	new_error = 0.35774 flipped
i = 228	new_error = 0.35296 added
i = 256	new_error = 0.34749 added
i = 301	new_error = 0.34493 added
i = 302	new_error = 0.34326 added
i = 303	new_error = 0.33065 added
i = 315	new_error = 0.32403 added
i = 349	new_error = 0.32344 added
i = 355	new_error = 0.32119 flipped
i = 481	new_error = 0.31966 added
i = 489	new_error = 0.31755 added
i = 503	new_error = 0.31231 added
i = 751	new_error = 0.31229 added
i = 753	new_error = 0.30748 added
i = 823	new_error = 0.30651 added
i = 829	new_error = 0.29657 added
i = 845	new_error = 0.29174 added
i = 1078	new_error = 0.29157 added
i = 1085	new_error = 0.28821 flipped
i = 1089	new_error = 0.28673 added
i = 1091	new_error = 0.28304 added
i = 1148	new_error = 0.28197 added
i = 1155	new_error = 0.27977 added
i = 1177	new_error = 0.27643 added
i = 1178	new_error = 0.27502 added
i = 1180	new_error = 0.27420 added
i = 1182	new_error = 0.27093 flipped
i = 1196	new_error = 0.27056 flipped
i = 1251	new_error = 0.26926 added
i = 1279	new_error = 0.26826 added
i = 1286	new_error = 0.26723 added
i = 1287	new_error = 0.26366 flipped
i = 1340	new_error = 0.26300 added
i = 1350	new_error = 0.26236 added
i = 1356	new_error = 0.26094 added
i = 1514	new_error = 0.25990 added
i = 1523	new_error = 0.25928 flipped
i = 1540	new_error = 0.25793 added
i = 1592	new_error = 0.25719 added
i = 1619	new_error = 0.25476 flipped
i = 1668	new_error = 0.25406 added
i = 1679	new_error = 0.25389 added
i = 1685	new_error = 0.25256 added
i = 1705	new_error = 0.25054 added
i = 1720	new_error = 0.24905 added
i = 1727	new_error = 0.24744 added
i = 1734	new_error = 0.24717 added
i = 1847	new_error = 0.24557 added
i = 1916	new_error = 0.24544 added
i = 1919	new_error = 0.24393 added
i = 2078	new_error = 0.24378 added
i = 2080	new_error = 0.24362 flipped
i = 2082	new_error = 0.24299 added
i = 2086	new_error = 0.23966 flipped
i = 2318	new_error = 0.23924 added
i = 2323	new_error = 0.23798 added
i = 2381	new_error = 0.23797 added
i = 2395	new_error = 0.23700 added
i = 2398	new_error = 0.23417 added
i = 2403	new_error = 0.23335 added
i = 2404	new_error = 0.23250 added
i = 2405	new_error = 0.23214 added
i = 2500	new_error = 0.23169 added
i = 2534	new_error = 0.23042 flipped
i = 2604	new_error = 0.23020 added
i = 2608	new_error = 0.23017 flipped
i = 2609	new_error = 0.23004 flipped
i = 2654	new_error = 0.22985 added
i = 2681	new_error = 0.22947 added
i = 2688	new_error = 0.22775 added
i = 2703	new_error = 0.22727 added
i = 2732	new_error = 0.22635 added
i = 2777	new_error = 0.22542 added
i = 2797	new_error = 0.22446 added
i = 2800	new_error = 0.22250 added
i = 2804	new_error = 0.22236 added
i = 2829	new_error = 0.22155 added
i = 2832	new_error = 0.21948 added
i = 2859	new_error = 0.21874 added
i = 2873	new_error = 0.21818 added
i = 2876	new_error = 0.21765 flipped
i = 2891	new_error = 0.21738 added
i = 3016	new_error = 0.21591 added
i = 3021	new_error = 0.21563 added
i = 3031	new_error = 0.21507 flipped
i = 3224	new_error = 0.21501 added
i = 3331	new_error = 0.21497 added
i = 3405	new_error = 0.21494 added
i = 3444	new_error = 0.21370 added
i = 3475	new_error = 0.21286 added
i = 3479	new_error = 0.21093 added
i = 3480	new_error = 0.20919 added
i = 3513	new_error = 0.20762 added
i = 3575	new_error = 0.20733 added
i = 3603	new_error = 0.20672 added
i = 3606	new_error = 0.20644 flipped
i = 3647	new_error = 0.20620 flipped
i = 3659	new_error = 0.20604 added
i = 3663	new_error = 0.20593 added
i = 3664	new_error = 0.20521 flipped
i = 3730	new_error = 0.20499 added
i = 3734	new_error = 0.20483 flipped
i = 3872	new_error = 0.20476 added
i = 3881	new_error = 0.20469 flipped
i = 3925	new_error = 0.20438 added
i = 3943	new_error = 0.20388 added
i = 3975	new_error = 0.20358 added
i = 3995	new_error = 0.20323 added
i = 4000	new_error = 0.20319 added
i = 4006	new_error = 0.20255 flipped
i = 4059	new_error = 0.20228 added
i = 4096	new_error = 0.20207 added
i = 4101	new_error = 0.20204 added
i = 4107	new_error = 0.20156 flipped
i = 4147	new_error = 0.20128 flipped
i = 4155	new_error = 0.20062 added
i = 4391	new_error = 0.20021 added
i = 4392	new_error = 0.19947 flipped
i = 4555	new_error = 0.19913 added
i = 4583	new_error = 0.19894 added
i = 4789	new_error = 0.19878 added
i = 4960	new_error = 0.19867 added
i = 4961	new_error = 0.19827 added
i = 4964	new_error = 0.19803 flipped
i = 5057	new_error = 0.19788 added
i = 5091	new_error = 0.19715 added
i = 5094	new_error = 0.19666 added
i = 5098	new_error = 0.19616 added
i = 5112	new_error = 0.19561 added
i = 5159	new_error = 0.19554 added
i = 5164	new_error = 0.19479 flipped
i = 5192	new_error = 0.19438 added
i = 5398	new_error = 0.19433 added
i = 5432	new_error = 0.19375 added
i = 5442	new_error = 0.19314 added
i = 5537	new_error = 0.19287 added
i = 5542	new_error = 0.19179 added
i = 5548	new_error = 0.19175 added
i = 5557	new_error = 0.19135 flipped
i = 5814	new_error = 0.19121 added
i = 6063	new_error = 0.19111 added
i = 6065	new_error = 0.19105 flipped
i = 6106	new_error = 0.19064 added
i = 6130	new_error = 0.19028 added
i = 6148	new_error = 0.19023 added
i = 6159	new_error = 0.19022 flipped
i = 6309	new_error = 0.19008 added
i = 6321	new_error = 0.18990 added
i = 6324	new_error = 0.18984 added
i = 6328	new_error = 0.18840 added
i = 6365	new_error = 0.18830 added
i = 6385	new_error = 0.18801 added
i = 6397	new_error = 0.18659 added
i = 6465	new_error = 0.18658 flipped
i = 6620	new_error = 0.18608 added
i = 6699	new_error = 0.18587 added
i = 6703	new_error = 0.18531 added
i = 6718	new_error = 0.18475 added
i = 6799	new_error = 0.18406 added
i = 6863	new_error = 0.18331 added
i = 6877	new_error = 0.18321 added
i = 6880	new_error = 0.18240 added
i = 7104	new_error = 0.18195 added
i = 7105	new_error = 0.18133 added
i = 7141	new_error = 0.18062 added
i = 7170	new_error = 0.18020 added
i = 7179	new_error = 0.17876 added
i = 7271	new_error = 0.17873 added
i = 7272	new_error = 0.17743 added
i = 7622	new_error = 0.17735 added
i = 7655	new_error = 0.17710 added
i = 7670	new_error = 0.17696 added
i = 7685	new_error = 0.17629 added
i = 7717	new_error = 0.17627 flipped
i = 7722	new_error = 0.17612 added
i = 7882	new_error = 0.17602 added
i = 7885	new_error = 0.17598 added
i = 7918	new_error = 0.17597 flipped
i = 7981	new_error = 0.17514 added
i = 8038	new_error = 0.17451 added
i = 8084	new_error = 0.17434 added
i = 8104	new_error = 0.17419 added
i = 8108	new_error = 0.17386 added
i = 8118	new_error = 0.17321 added
i = 8266	new_error = 0.17284 added
i = 8298	new_error = 0.17240 added
i = 8323	new_error = 0.17240 added
i = 8326	new_error = 0.17223 added
i = 8365	new_error = 0.17213 added
i = 8368	new_error = 0.17168 flipped
i = 8420	new_error = 0.17166 added
i = 8423	new_error = 0.17054 added
i = 8752	new_error = 0.17040 added
i = 8777	new_error = 0.17022 flipped
i = 8834	new_error = 0.17002 added
i = 9294	new_error = 0.16983 added
i = 9321	new_error = 0.16961 flipped
i = 9345	new_error = 0.16957 added
i = 9372	new_error = 0.16947 flipped
i = 9395	new_error = 0.16946 added
i = 9487	new_error = 0.16869 added
i = 9496	new_error = 0.16831 added
i = 9508	new_error = 0.16770 added
i = 9557	new_error = 0.16758 added
i = 9570	new_error = 0.16721 added
i = 9599	new_error = 0.16703 added
i = 9625	new_error = 0.16635 added
i = 9630	new_error = 0.16539 added
i = 9668	new_error = 0.16444 added
i = 9699	new_error = 0.16439 flipped
i = 9742	new_error = 0.16434 added
i = 9884	new_error = 0.16419 flipped
i = 9887	new_error = 0.16391 added
i = 9971	new_error = 0.16380 added
i = 9976	new_error = 0.16360 flipped
i = 10275	new_error = 0.16335 added
i = 10276	new_error = 0.16237 added
i = 10513	new_error = 0.16235 added
i = 10562	new_error = 0.16202 added
i = 10575	new_error = 0.16196 added
i = 10623	new_error = 0.16167 added
i = 10641	new_error = 0.16137 added
i = 10715	new_error = 0.16130 added
i = 10716	new_error = 0.16086 added
i = 10843	new_error = 0.16062 added
i = 10853	new_error = 0.15967 added
i = 10993	new_error = 0.15944 flipped
i = 11013	new_error = 0.15920 flipped
i = 11180	new_error = 0.15893 added
i = 11192	new_error = 0.15887 added
i = 11342	new_error = 0.15886 added
i = 11386	new_error = 0.15876 added
i = 11407	new_error = 0.15874 added
i = 11415	new_error = 0.15867 added
i = 11449	new_error = 0.15842 added
i = 11489	new_error = 0.15817 added
i = 11491	new_error = 0.15799 added
i = 11523	new_error = 0.15776 added
i = 11553	new_error = 0.15733 added
i = 11768	new_error = 0.15720 added
i = 11957	new_error = 0.15687 added
i = 11976	new_error = 0.15659 added
i = 12188	new_error = 0.15645 added
i = 12205	new_error = 0.15638 flipped
i = 12247	new_error = 0.15613 added
i = 12281	new_error = 0.15602 added
i = 12304	new_error = 0.15598 added
i = 12387	new_error = 0.15550 added
0.80748
Loading the imdb reviews data
Data loaded.
Extracting features from the training dataset using a sparse vectorizer
Feature extraction technique is CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=5,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None).
done in 8.657356023788452s
n_samples: 12500, n_features: 19271 

Loading took 10.23s.

i = 816	new error = 0.15540
i = 890	new error = 0.15540
i = 992	new error = 0.15525
i = 1287	new error = 0.15501
i = 1364	new error = 0.15490
i = 1514	new error = 0.15489
i = 1590	new error = 0.15469
i = 2087	new error = 0.15467
i = 2112	new error = 0.15425
i = 2197	new error = 0.15422
i = 2247	new error = 0.15415
i = 2249	new error = 0.15369
i = 2707	new error = 0.15345
i = 3174	new error = 0.15338
i = 3195	new error = 0.15254
i = 3232	new error = 0.15201
i = 3331	new error = 0.15187
i = 3616	new error = 0.15178
i = 3684	new error = 0.15173
i = 3685	new error = 0.15159
i = 3706	new error = 0.15116
i = 3707	new error = 0.15083
i = 3718	new error = 0.15052
i = 3809	new error = 0.15051
i = 3822	new error = 0.15039
i = 3882	new error = 0.15019
i = 3903	new error = 0.15019
i = 3905	new error = 0.14996
i = 3943	new error = 0.14996
i = 4583	new error = 0.14995
i = 4587	new error = 0.14957
i = 4606	new error = 0.14927
i = 4648	new error = 0.14913
i = 4696	new error = 0.14900
i = 4770	new error = 0.14876
i = 4800	new error = 0.14871
i = 5473	new error = 0.14869
i = 5491	new error = 0.14849
i = 5504	new error = 0.14843
i = 5508	new error = 0.14839
i = 5842	new error = 0.14825
i = 5872	new error = 0.14812
i = 6159	new error = 0.14805
i = 6187	new error = 0.14799
i = 6261	new error = 0.14785
i = 6364	new error = 0.14775
i = 6445	new error = 0.14724
i = 7158	new error = 0.14701
i = 7322	new error = 0.14677
i = 7323	new error = 0.14676
i = 7335	new error = 0.14673
i = 7724	new error = 0.14670
i = 7975	new error = 0.14660
i = 8056	new error = 0.14639
i = 8114	new error = 0.14629
i = 8119	new error = 0.14608
i = 8164	new error = 0.14603
i = 8190	new error = 0.14594
i = 8209	new error = 0.14579
i = 8252	new error = 0.14573
i = 8323	new error = 0.14563
i = 8326	new error = 0.14552
i = 8497	new error = 0.14551
i = 8631	new error = 0.14547
i = 8658	new error = 0.14525
i = 8777	new error = 0.14518
i = 8816	new error = 0.14500
i = 8832	new error = 0.14470
i = 8849	new error = 0.14462
i = 8856	new error = 0.14450
i = 8993	new error = 0.14437
i = 8997	new error = 0.14427
i = 9028	new error = 0.14397
i = 9030	new error = 0.14389
i = 9037	new error = 0.14354
i = 9055	new error = 0.14352
i = 9058	new error = 0.14282
i = 9117	new error = 0.14272
i = 9136	new error = 0.14238
i = 9206	new error = 0.14218
i = 9570	new error = 0.14213
i = 9699	new error = 0.14211
i = 9742	new error = 0.14211
i = 10392	new error = 0.14203
i = 10461	new error = 0.14189
i = 10464	new error = 0.14185
i = 10475	new error = 0.14161
i = 10509	new error = 0.14158
i = 10517	new error = 0.14124
i = 10523	new error = 0.14123
i = 10524	new error = 0.14099
i = 10622	new error = 0.14093
i = 10636	new error = 0.14086
i = 10688	new error = 0.14070
i = 10773	new error = 0.14061
i = 10832	new error = 0.14043
i = 10855	new error = 0.14014
i = 10872	new error = 0.14006
i = 10884	new error = 0.14001
i = 10891	new error = 0.13963
i = 10892	new error = 0.13960
i = 10897	new error = 0.13957
i = 10915	new error = 0.13952
i = 10920	new error = 0.13938
i = 10945	new error = 0.13934
i = 11016	new error = 0.13920
i = 11095	new error = 0.13902
i = 11103	new error = 0.13887
i = 11537	new error = 0.13861
i = 11558	new error = 0.13858
i = 11586	new error = 0.13830
i = 11620	new error = 0.13810
i = 11629	new error = 0.13791
i = 11631	new error = 0.13782
i = 11682	new error = 0.13753
i = 11685	new error = 0.13749
i = 11711	new error = 0.13717
i = 11739	new error = 0.13699
i = 11740	new error = 0.13672
i = 11841	new error = 0.13670
i = 11862	new error = 0.13660
i = 11899	new error = 0.13659
i = 12010	new error = 0.13639
i = 12036	new error = 0.13635
i = 12040	new error = 0.13609
i = 12048	new error = 0.13581
i = 12327	new error = 0.13570
i = 12333	new error = 0.13553
i = 12337	new error = 0.13526
i = 12424	new error = 0.13514
i = 12482	new error = 0.13507
0.82324
Loading the imdb reviews data
Data loaded.
Extracting features from the training dataset using a sparse vectorizer
Feature extraction technique is CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=5,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None).
done in 8.621003150939941s
n_samples: 12500, n_features: 19271 

Loading took 10.18s.

i = 71	new error = 0.13503
i = 136	new error = 0.13489
i = 256	new error = 0.13489
i = 260	new error = 0.13472
i = 263	new error = 0.13450
i = 283	new error = 0.13449
i = 330	new error = 0.13442
i = 573	new error = 0.13432
i = 574	new error = 0.13395
i = 588	new error = 0.13379
i = 594	new error = 0.13328
i = 753	new error = 0.13319
i = 823	new error = 0.13319
i = 842	new error = 0.13310
i = 875	new error = 0.13308
i = 895	new error = 0.13305
i = 896	new error = 0.13298
i = 905	new error = 0.13290
i = 1073	new error = 0.13285
i = 1083	new error = 0.13253
i = 1111	new error = 0.13243
i = 1115	new error = 0.13217
i = 1148	new error = 0.13214
i = 1233	new error = 0.13212
i = 1235	new error = 0.13189
i = 1394	new error = 0.13188
i = 1508	new error = 0.13177
i = 1512	new error = 0.13163
i = 1514	new error = 0.13152
i = 1596	new error = 0.13151
i = 1603	new error = 0.13114
i = 1619	new error = 0.13105
i = 1685	new error = 0.13098
i = 1805	new error = 0.13093
i = 1831	new error = 0.13093
i = 1919	new error = 0.13078
i = 2218	new error = 0.13068
i = 2323	new error = 0.13045
i = 2352	new error = 0.13044
i = 2441	new error = 0.13041
i = 2500	new error = 0.13041
i = 2835	new error = 0.13027
i = 3095	new error = 0.13025
i = 3126	new error = 0.13009
i = 3136	new error = 0.12992
i = 3146	new error = 0.12979
i = 3160	new error = 0.12964
i = 3292	new error = 0.12959
i = 3306	new error = 0.12953
i = 3312	new error = 0.12937
i = 3331	new error = 0.12933
i = 3405	new error = 0.12928
i = 3786	new error = 0.12902
i = 3819	new error = 0.12884
i = 3987	new error = 0.12876
i = 4310	new error = 0.12864
i = 4327	new error = 0.12859
i = 5112	new error = 0.12852
i = 5149	new error = 0.12846
i = 5164	new error = 0.12808
i = 5233	new error = 0.12796
i = 5237	new error = 0.12789
i = 5337	new error = 0.12786
i = 5343	new error = 0.12775
i = 5349	new error = 0.12758
i = 5395	new error = 0.12755
i = 5449	new error = 0.12753
i = 6546	new error = 0.12748
i = 6561	new error = 0.12717
i = 6699	new error = 0.12717
i = 6939	new error = 0.12708
i = 6946	new error = 0.12696
i = 6964	new error = 0.12693
i = 7033	new error = 0.12690
i = 7058	new error = 0.12653
i = 7087	new error = 0.12632
i = 7243	new error = 0.12627
i = 7575	new error = 0.12626
i = 7589	new error = 0.12610
i = 8252	new error = 0.12608
i = 8288	new error = 0.12602
i = 8289	new error = 0.12600
i = 8360	new error = 0.12584
i = 8395	new error = 0.12582
i = 8443	new error = 0.12581
i = 8447	new error = 0.12574
i = 8468	new error = 0.12562
i = 8476	new error = 0.12561
i = 8532	new error = 0.12548
i = 8536	new error = 0.12542
i = 8560	new error = 0.12525
i = 8717	new error = 0.12519
i = 8759	new error = 0.12516
i = 8772	new error = 0.12513
i = 8834	new error = 0.12511
i = 8846	new error = 0.12501
i = 8855	new error = 0.12485
i = 8906	new error = 0.12470
i = 8977	new error = 0.12460
i = 8978	new error = 0.12446
i = 9004	new error = 0.12395
i = 9254	new error = 0.12379
i = 9321	new error = 0.12371
i = 9328	new error = 0.12366
i = 9335	new error = 0.12352
i = 9372	new error = 0.12341
i = 9473	new error = 0.12340
i = 9475	new error = 0.12328
i = 9485	new error = 0.12323
i = 9512	new error = 0.12319
i = 9559	new error = 0.12309
i = 9570	new error = 0.12294
i = 9742	new error = 0.12291
i = 9900	new error = 0.12275
i = 9963	new error = 0.12270
i = 9976	new error = 0.12261
i = 10011	new error = 0.12253
i = 10052	new error = 0.12248
i = 10058	new error = 0.12243
i = 10397	new error = 0.12229
i = 10511	new error = 0.12199
i = 10705	new error = 0.12173
i = 10718	new error = 0.12156
i = 10945	new error = 0.12154
i = 10961	new error = 0.12148
i = 11065	new error = 0.12147
i = 11089	new error = 0.12138
i = 11108	new error = 0.12119
i = 11113	new error = 0.12104
i = 11115	new error = 0.12090
i = 11128	new error = 0.12072
i = 11160	new error = 0.12070
i = 11161	new error = 0.12053
i = 11170	new error = 0.12045
i = 11172	new error = 0.12036
i = 11201	new error = 0.12024
i = 11208	new error = 0.12017
i = 11306	new error = 0.12000
i = 11326	new error = 0.12000
i = 11384	new error = 0.11974
i = 11397	new error = 0.11973
i = 11401	new error = 0.11970
i = 11407	new error = 0.11923
i = 11454	new error = 0.11907
i = 11491	new error = 0.11904
i = 11537	new error = 0.11901
i = 11930	new error = 0.11893
i = 11948	new error = 0.11890
i = 11961	new error = 0.11881
i = 11979	new error = 0.11852
i = 12106	new error = 0.11826
i = 12192	new error = 0.11824
0.83088
Loading the imdb reviews data
Data loaded.
Extracting features from the training dataset using a sparse vectorizer
Feature extraction technique is CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=5,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None).
done in 8.498327732086182s
n_samples: 12500, n_features: 19271 

Loading took 10.04s.

i = 143	new error = 0.11802
i = 162	new error = 0.11790
i = 168	new error = 0.11768
i = 224	new error = 0.11749
i = 811	new error = 0.11748
i = 823	new error = 0.11746
i = 845	new error = 0.11741
i = 905	new error = 0.11739
i = 919	new error = 0.11713
i = 1115	new error = 0.11706
i = 1134	new error = 0.11688
i = 1447	new error = 0.11684
i = 1648	new error = 0.11683
i = 1665	new error = 0.11682
i = 1679	new error = 0.11682
i = 1747	new error = 0.11669
i = 1755	new error = 0.11669
i = 1881	new error = 0.11668
i = 1933	new error = 0.11665
i = 1952	new error = 0.11654
i = 1969	new error = 0.11648
i = 1978	new error = 0.11627
i = 1989	new error = 0.11615
i = 2016	new error = 0.11614
i = 2025	new error = 0.11591
i = 2086	new error = 0.11566
i = 2087	new error = 0.11565
i = 2111	new error = 0.11536
i = 2114	new error = 0.11508
i = 2193	new error = 0.11507
i = 2197	new error = 0.11504
i = 2217	new error = 0.11495
i = 2304	new error = 0.11460
i = 2320	new error = 0.11458
i = 2445	new error = 0.11453
i = 2454	new error = 0.11430
i = 2500	new error = 0.11426
i = 2601	new error = 0.11412
i = 2608	new error = 0.11402
i = 2732	new error = 0.11401
i = 2797	new error = 0.11399
i = 2800	new error = 0.11383
i = 2840	new error = 0.11381
i = 2869	new error = 0.11373
i = 2970	new error = 0.11369
i = 3039	new error = 0.11361
i = 3050	new error = 0.11353
i = 3055	new error = 0.11352
i = 3080	new error = 0.11346
i = 3082	new error = 0.11345
i = 3405	new error = 0.11345
i = 3442	new error = 0.11344
i = 3444	new error = 0.11317
i = 3529	new error = 0.11315
i = 3616	new error = 0.11311
i = 3652	new error = 0.11304
i = 3664	new error = 0.11271
i = 3722	new error = 0.11265
i = 3849	new error = 0.11264
i = 3866	new error = 0.11255
i = 3872	new error = 0.11250
i = 3922	new error = 0.11247
i = 3943	new error = 0.11246
i = 4059	new error = 0.11244
i = 4081	new error = 0.11239
i = 4147	new error = 0.11216
i = 4192	new error = 0.11216
i = 4359	new error = 0.11213
i = 4775	new error = 0.11209
i = 4800	new error = 0.11202
i = 4864	new error = 0.11196
i = 4866	new error = 0.11172
i = 5192	new error = 0.11169
i = 5224	new error = 0.11165
i = 5388	new error = 0.11149
i = 5424	new error = 0.11135
i = 5436	new error = 0.11115
i = 5442	new error = 0.11111
i = 5518	new error = 0.11102
i = 5523	new error = 0.11088
i = 5661	new error = 0.11076
i = 5885	new error = 0.11067
i = 5902	new error = 0.11042
i = 5903	new error = 0.11039
i = 6048	new error = 0.11032
i = 6054	new error = 0.11023
i = 6055	new error = 0.11020
i = 6186	new error = 0.11017
i = 6187	new error = 0.11002
i = 6524	new error = 0.10997
i = 6545	new error = 0.10995
i = 6618	new error = 0.10984
i = 6675	new error = 0.10983
i = 6688	new error = 0.10963
i = 6698	new error = 0.10947
i = 6699	new error = 0.10939
i = 6786	new error = 0.10927
i = 7192	new error = 0.10927
i = 7195	new error = 0.10907
i = 7200	new error = 0.10901
i = 7243	new error = 0.10901
i = 7281	new error = 0.10892
i = 7463	new error = 0.10890
i = 7476	new error = 0.10879
i = 7502	new error = 0.10873
i = 7693	new error = 0.10865
i = 7695	new error = 0.10860
i = 7722	new error = 0.10852
i = 7724	new error = 0.10843
i = 8557	new error = 0.10839
i = 8671	new error = 0.10836
i = 8834	new error = 0.10835
i = 8842	new error = 0.10824
i = 9426	new error = 0.10816
i = 9547	new error = 0.10813
i = 9745	new error = 0.10811
i = 9842	new error = 0.10810
i = 9844	new error = 0.10809
i = 9867	new error = 0.10806
i = 10058	new error = 0.10803
i = 10145	new error = 0.10792
i = 10320	new error = 0.10786
i = 10333	new error = 0.10784
i = 10338	new error = 0.10778
i = 10358	new error = 0.10774
i = 10471	new error = 0.10770
i = 10679	new error = 0.10766
i = 10945	new error = 0.10763
i = 11128	new error = 0.10760
i = 11132	new error = 0.10760
i = 11299	new error = 0.10760
i = 11326	new error = 0.10747
i = 11443	new error = 0.10741
i = 11537	new error = 0.10731
i = 12133	new error = 0.10729
i = 12366	new error = 0.10725
i = 12380	new error = 0.10723
0.83684
Loading the imdb reviews data
Data loaded.
Extracting features from the training dataset using a sparse vectorizer
Feature extraction technique is CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=5,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None).
done in 8.52981185913086s
n_samples: 12500, n_features: 19271 

Loading took 10.07s.

i = 120	new error = 0.10713
i = 224	new error = 0.10709
i = 227	new error = 0.10701
i = 228	new error = 0.10697
i = 256	new error = 0.10695
i = 336	new error = 0.10692
i = 444	new error = 0.10690
i = 455	new error = 0.10677
i = 823	new error = 0.10671
i = 854	new error = 0.10669
i = 870	new error = 0.10659
i = 969	new error = 0.10644
i = 1148	new error = 0.10636
i = 1446	new error = 0.10626
i = 1755	new error = 0.10621
i = 1767	new error = 0.10611
i = 1782	new error = 0.10607
i = 1783	new error = 0.10587
i = 1803	new error = 0.10576
i = 1822	new error = 0.10574
i = 1915	new error = 0.10572
i = 1952	new error = 0.10568
i = 2110	new error = 0.10551
i = 2405	new error = 0.10548
i = 2546	new error = 0.10548
i = 2561	new error = 0.10540
i = 2609	new error = 0.10535
i = 2654	new error = 0.10531
i = 2707	new error = 0.10530
i = 2903	new error = 0.10529
i = 3031	new error = 0.10521
i = 3074	new error = 0.10517
i = 3125	new error = 0.10516
i = 3146	new error = 0.10515
i = 3160	new error = 0.10514
i = 3405	new error = 0.10513
i = 4006	new error = 0.10512
i = 4255	new error = 0.10499
i = 4313	new error = 0.10490
i = 4393	new error = 0.10480
i = 4685	new error = 0.10472
i = 4775	new error = 0.10472
i = 4797	new error = 0.10470
i = 4800	new error = 0.10466
i = 4953	new error = 0.10465
i = 5162	new error = 0.10456
i = 5192	new error = 0.10454
i = 5343	new error = 0.10453
i = 5432	new error = 0.10445
i = 5500	new error = 0.10445
i = 5564	new error = 0.10432
i = 5583	new error = 0.10423
i = 5633	new error = 0.10412
i = 5654	new error = 0.10410
i = 5815	new error = 0.10408
i = 5826	new error = 0.10406
i = 6055	new error = 0.10400
i = 6091	new error = 0.10400
i = 6385	new error = 0.10391
i = 6397	new error = 0.10389
i = 6521	new error = 0.10384
i = 6525	new error = 0.10384
i = 6656	new error = 0.10377
i = 6689	new error = 0.10370
i = 6783	new error = 0.10368
i = 6933	new error = 0.10366
i = 7593	new error = 0.10361
i = 7606	new error = 0.10358
i = 7613	new error = 0.10347
i = 7615	new error = 0.10345
i = 7625	new error = 0.10333
i = 8252	new error = 0.10330
i = 8570	new error = 0.10320
i = 8772	new error = 0.10308
i = 9087	new error = 0.10304
i = 9117	new error = 0.10304
i = 9131	new error = 0.10298
i = 9142	new error = 0.10297
i = 9146	new error = 0.10296
i = 9246	new error = 0.10285
i = 9395	new error = 0.10283
i = 9418	new error = 0.10278
i = 9928	new error = 0.10275
i = 9976	new error = 0.10273
i = 10055	new error = 0.10265
i = 10058	new error = 0.10243
i = 10071	new error = 0.10242
i = 10087	new error = 0.10240
i = 10117	new error = 0.10230
i = 10248	new error = 0.10229
i = 10295	new error = 0.10226
i = 10316	new error = 0.10222
i = 10341	new error = 0.10217
i = 10378	new error = 0.10208
i = 10395	new error = 0.10207
i = 10562	new error = 0.10201
i = 10679	new error = 0.10201
i = 10732	new error = 0.10199
i = 10737	new error = 0.10185
i = 10761	new error = 0.10182
i = 10773	new error = 0.10181
i = 11022	new error = 0.10166
i = 11115	new error = 0.10159
i = 11132	new error = 0.10154
i = 11160	new error = 0.10144
0.8384
Loading the imdb reviews data
Data loaded.
Extracting features from the training dataset using a sparse vectorizer
Feature extraction technique is CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=5,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None).
done in 8.49595022201538s
n_samples: 12500, n_features: 19271 

Loading took 10.02s.

i = 782	new error = 0.10138
i = 811	new error = 0.10128
i = 992	new error = 0.10124
i = 1379	new error = 0.10115
i = 1381	new error = 0.10106
i = 1952	new error = 0.10104
i = 1953	new error = 0.10101
i = 2315	new error = 0.10100
i = 2323	new error = 0.10093
i = 2654	new error = 0.10091
i = 4130	new error = 0.10090
i = 4775	new error = 0.10082
i = 4789	new error = 0.10076
i = 4811	new error = 0.10072
i = 4966	new error = 0.10067
i = 4981	new error = 0.10064
i = 5192	new error = 0.10061
i = 5209	new error = 0.10058
i = 5230	new error = 0.10053
i = 5500	new error = 0.10051
i = 5539	new error = 0.10050
i = 5691	new error = 0.10050
i = 5829	new error = 0.10046
i = 5863	new error = 0.10020
i = 6237	new error = 0.10018
i = 6316	new error = 0.10014
i = 6318	new error = 0.09993
i = 6379	new error = 0.09992
i = 7476	new error = 0.09991
i = 7502	new error = 0.09991
i = 7931	new error = 0.09983
i = 8371	new error = 0.09976
i = 8689	new error = 0.09971
i = 8717	new error = 0.09970
i = 8777	new error = 0.09966
i = 8914	new error = 0.09964
i = 8920	new error = 0.09948
i = 9146	new error = 0.09945
i = 9148	new error = 0.09939
i = 9254	new error = 0.09937
i = 9343	new error = 0.09934
i = 9360	new error = 0.09931
i = 9454	new error = 0.09928
i = 9894	new error = 0.09928
i = 10036	new error = 0.09927
i = 10117	new error = 0.09923
i = 10211	new error = 0.09918
i = 10347	new error = 0.09912
i = 10361	new error = 0.09910
i = 10397	new error = 0.09902
i = 10442	new error = 0.09900
i = 10504	new error = 0.09895
i = 10549	new error = 0.09878
i = 10564	new error = 0.09870
i = 10574	new error = 0.09859
i = 10641	new error = 0.09855
i = 11011	new error = 0.09851
i = 11052	new error = 0.09849
i = 11088	new error = 0.09844
i = 11126	new error = 0.09842
i = 11164	new error = 0.09839
i = 11172	new error = 0.09835
i = 11756	new error = 0.09835
i = 11860	new error = 0.09830
i = 11898	new error = 0.09828
i = 11914	new error = 0.09818
i = 11948	new error = 0.09816
i = 12121	new error = 0.09798
i = 12281	new error = 0.09796
i = 12372	new error = 0.09794
i = 12379	new error = 0.09780
0.84108
Loading the imdb reviews data
Data loaded.
Extracting features from the training dataset using a sparse vectorizer
Feature extraction technique is CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=5,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None).
done in 8.62629222869873s
n_samples: 12500, n_features: 19271 

Loading took 10.16s.

i = 63	new error = 0.09778
i = 145	new error = 0.09777
i = 192	new error = 0.09763
i = 256	new error = 0.09756
i = 329	new error = 0.09755
i = 514	new error = 0.09753
i = 592	new error = 0.09751
i = 617	new error = 0.09750
i = 884	new error = 0.09743
i = 944	new error = 0.09735
i = 1085	new error = 0.09729
i = 1297	new error = 0.09721
i = 1465	new error = 0.09715
i = 1600	new error = 0.09713
i = 2434	new error = 0.09711
i = 2551	new error = 0.09688
i = 2568	new error = 0.09679
i = 2573	new error = 0.09664
i = 2575	new error = 0.09653
i = 2609	new error = 0.09647
i = 2707	new error = 0.09647
i = 2773	new error = 0.09646
i = 2788	new error = 0.09643
i = 3031	new error = 0.09637
i = 3224	new error = 0.09635
i = 3358	new error = 0.09633
i = 3395	new error = 0.09632
i = 3435	new error = 0.09631
i = 3442	new error = 0.09623
i = 3866	new error = 0.09622
i = 3975	new error = 0.09621
i = 3983	new error = 0.09617
i = 4155	new error = 0.09615
i = 4453	new error = 0.09615
i = 4462	new error = 0.09607
i = 4571	new error = 0.09607
i = 4694	new error = 0.09606
i = 6246	new error = 0.09604
i = 6261	new error = 0.09589
i = 6262	new error = 0.09588
i = 6379	new error = 0.09583
i = 6588	new error = 0.09578
i = 6630	new error = 0.09578
i = 7105	new error = 0.09578
i = 7231	new error = 0.09573
i = 7410	new error = 0.09571
i = 7414	new error = 0.09568
i = 7453	new error = 0.09564
i = 7480	new error = 0.09551
i = 7722	new error = 0.09534
i = 8039	new error = 0.09532
i = 8073	new error = 0.09530
i = 8214	new error = 0.09528
i = 8323	new error = 0.09516
i = 8468	new error = 0.09513
i = 8717	new error = 0.09503
i = 8810	new error = 0.09501
i = 8976	new error = 0.09496
i = 9025	new error = 0.09495
i = 9059	new error = 0.09494
i = 9194	new error = 0.09488
i = 9204	new error = 0.09486
i = 9291	new error = 0.09478
i = 9297	new error = 0.09478
i = 9307	new error = 0.09476
i = 9372	new error = 0.09476
i = 9387	new error = 0.09468
i = 9866	new error = 0.09466
i = 9908	new error = 0.09466
i = 9924	new error = 0.09459
i = 9963	new error = 0.09453
i = 10184	new error = 0.09450
i = 10235	new error = 0.09450
i = 10392	new error = 0.09440
i = 10636	new error = 0.09437
i = 10655	new error = 0.09418
i = 10657	new error = 0.09395
i = 10660	new error = 0.09393
i = 10662	new error = 0.09386
i = 10868	new error = 0.09385
i = 10891	new error = 0.09381
i = 11132	new error = 0.09377
i = 11491	new error = 0.09377
i = 11537	new error = 0.09376
i = 11685	new error = 0.09369
i = 11743	new error = 0.09366
i = 11779	new error = 0.09365
i = 11802	new error = 0.09360
i = 11908	new error = 0.09355
i = 11930	new error = 0.09355
i = 11947	new error = 0.09353
i = 11948	new error = 0.09347
i = 11949	new error = 0.09346
i = 12437	new error = 0.09346
i = 12441	new error = 0.09341
i = 12480	new error = 0.09337
0.84404
Loading the imdb reviews data
Data loaded.
Extracting features from the training dataset using a sparse vectorizer
Feature extraction technique is CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=5,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None).
done in 8.492767095565796s
n_samples: 12500, n_features: 19271 

Loading took 10.03s.

i = 355	new error = 0.09328
i = 371	new error = 0.09318
i = 408	new error = 0.09313
i = 475	new error = 0.09312
i = 481	new error = 0.09309
i = 641	new error = 0.09306
i = 707	new error = 0.09303
i = 762	new error = 0.09303
i = 763	new error = 0.09300
i = 811	new error = 0.09297
i = 1045	new error = 0.09297
i = 1047	new error = 0.09284
i = 1050	new error = 0.09279
i = 1055	new error = 0.09275
i = 1207	new error = 0.09268
i = 1230	new error = 0.09242
i = 1590	new error = 0.09237
i = 2376	new error = 0.09233
i = 2379	new error = 0.09227
i = 2441	new error = 0.09218
i = 2446	new error = 0.09212
i = 2463	new error = 0.09202
i = 2500	new error = 0.09199
i = 2534	new error = 0.09196
i = 2609	new error = 0.09190
i = 2707	new error = 0.09188
i = 2773	new error = 0.09186
i = 2969	new error = 0.09184
i = 3039	new error = 0.09184
i = 3146	new error = 0.09183
i = 3224	new error = 0.09182
i = 3367	new error = 0.09170
i = 3400	new error = 0.09168
i = 3682	new error = 0.09165
i = 3911	new error = 0.09161
i = 3962	new error = 0.09151
i = 4006	new error = 0.09147
i = 4294	new error = 0.09147
i = 4363	new error = 0.09143
i = 4417	new error = 0.09139
i = 4447	new error = 0.09134
i = 4453	new error = 0.09132
i = 4571	new error = 0.09128
i = 4668	new error = 0.09125
i = 4789	new error = 0.09115
i = 4802	new error = 0.09105
i = 4838	new error = 0.09104
i = 4844	new error = 0.09104
i = 4967	new error = 0.09103
i = 5098	new error = 0.09094
i = 5230	new error = 0.09089
i = 5398	new error = 0.09086
i = 5442	new error = 0.09085
i = 5548	new error = 0.09069
i = 5551	new error = 0.09065
i = 6181	new error = 0.09058
i = 6235	new error = 0.09052
i = 6238	new error = 0.09051
i = 6477	new error = 0.09047
i = 6591	new error = 0.09036
i = 6600	new error = 0.09035
i = 6605	new error = 0.09022
i = 6693	new error = 0.09019
i = 6776	new error = 0.09011
i = 6785	new error = 0.09006
i = 6811	new error = 0.09003
i = 6918	new error = 0.08996
i = 6947	new error = 0.08991
i = 7007	new error = 0.08989
i = 7024	new error = 0.08987
i = 7032	new error = 0.08986
i = 7044	new error = 0.08981
i = 7074	new error = 0.08976
i = 7119	new error = 0.08975
i = 7132	new error = 0.08962
i = 7205	new error = 0.08955
i = 7281	new error = 0.08951
i = 7393	new error = 0.08951
i = 7471	new error = 0.08950
i = 7537	new error = 0.08942
i = 7567	new error = 0.08940
i = 7814	new error = 0.08939
i = 7850	new error = 0.08935
i = 7975	new error = 0.08934
i = 7988	new error = 0.08930
i = 8189	new error = 0.08928
i = 8214	new error = 0.08923
i = 8237	new error = 0.08915
i = 8238	new error = 0.08899
i = 8435	new error = 0.08899
i = 8458	new error = 0.08891
i = 8552	new error = 0.08890
i = 8563	new error = 0.08889
i = 8802	new error = 0.08886
i = 8816	new error = 0.08885
i = 8887	new error = 0.08885
i = 9068	new error = 0.08881
i = 9321	new error = 0.08880
i = 9476	new error = 0.08878
i = 9484	new error = 0.08873
i = 9508	new error = 0.08873
i = 9547	new error = 0.08872
i = 9597	new error = 0.08861
i = 9728	new error = 0.08857
i = 9756	new error = 0.08856
i = 9967	new error = 0.08856
i = 9976	new error = 0.08856
i = 10096	new error = 0.08849
i = 10148	new error = 0.08848
i = 10180	new error = 0.08845
i = 10341	new error = 0.08844
i = 10397	new error = 0.08835
i = 10479	new error = 0.08832
i = 10542	new error = 0.08830
i = 10594	new error = 0.08829
i = 10636	new error = 0.08827
i = 11125	new error = 0.08810
i = 11210	new error = 0.08809
i = 11211	new error = 0.08807
i = 11445	new error = 0.08803
i = 11463	new error = 0.08798
i = 11541	new error = 0.08768
i = 11564	new error = 0.08764
i = 11604	new error = 0.08753
i = 11649	new error = 0.08753
i = 12387	new error = 0.08750
0.8464
Loading the imdb reviews data
Data loaded.
Extracting features from the training dataset using a sparse vectorizer
Feature extraction technique is CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=5,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None).
done in 8.54263687133789s
n_samples: 12500, n_features: 19271 

Loading took 10.09s.

i = 34	new error = 0.08739
i = 224	new error = 0.08737
i = 228	new error = 0.08734
i = 363	new error = 0.08728
i = 402	new error = 0.08711
i = 726	new error = 0.08710
i = 744	new error = 0.08710
i = 815	new error = 0.08700
i = 816	new error = 0.08696
i = 1014	new error = 0.08690
i = 1042	new error = 0.08687
i = 1110	new error = 0.08682
i = 1171	new error = 0.08678
i = 1286	new error = 0.08666
i = 1371	new error = 0.08666
i = 1386	new error = 0.08666
i = 1540	new error = 0.08666
i = 1955	new error = 0.08664
i = 1969	new error = 0.08663
i = 2032	new error = 0.08663
i = 2087	new error = 0.08660
i = 2197	new error = 0.08659
i = 2260	new error = 0.08654
i = 2293	new error = 0.08653
i = 2362	new error = 0.08645
i = 2408	new error = 0.08634
i = 2411	new error = 0.08615
i = 2608	new error = 0.08615
i = 2707	new error = 0.08608
i = 2848	new error = 0.08604
i = 3031	new error = 0.08602
i = 3102	new error = 0.08591
i = 3125	new error = 0.08584
i = 3224	new error = 0.08583
i = 3400	new error = 0.08579
i = 3490	new error = 0.08574
i = 3493	new error = 0.08573
i = 3701	new error = 0.08571
i = 3730	new error = 0.08570
i = 3917	new error = 0.08569
i = 3937	new error = 0.08564
i = 3962	new error = 0.08563
i = 3975	new error = 0.08557
i = 3979	new error = 0.08552
i = 4046	new error = 0.08548
i = 4181	new error = 0.08546
i = 4392	new error = 0.08544
i = 4456	new error = 0.08543
i = 4462	new error = 0.08535
i = 4563	new error = 0.08531
i = 4841	new error = 0.08529
i = 4899	new error = 0.08525
i = 4932	new error = 0.08524
i = 4966	new error = 0.08524
i = 5040	new error = 0.08519
i = 5075	new error = 0.08519
i = 5398	new error = 0.08516
i = 5500	new error = 0.08515
i = 5661	new error = 0.08513
i = 5757	new error = 0.08505
i = 5855	new error = 0.08502
i = 6104	new error = 0.08497
i = 6111	new error = 0.08493
i = 6364	new error = 0.08484
i = 6379	new error = 0.08484
i = 6397	new error = 0.08483
i = 6415	new error = 0.08481
i = 6447	new error = 0.08461
i = 6509	new error = 0.08449
i = 6561	new error = 0.08444
i = 6602	new error = 0.08441
i = 6610	new error = 0.08440
i = 6641	new error = 0.08439
i = 6691	new error = 0.08434
i = 6753	new error = 0.08432
i = 6754	new error = 0.08429
i = 6956	new error = 0.08423
i = 6962	new error = 0.08421
i = 6976	new error = 0.08421
i = 6984	new error = 0.08420
i = 7082	new error = 0.08419
i = 7092	new error = 0.08412
i = 7141	new error = 0.08401
i = 7152	new error = 0.08397
i = 7319	new error = 0.08382
i = 7491	new error = 0.08377
i = 7693	new error = 0.08375
i = 7772	new error = 0.08374
i = 7785	new error = 0.08372
i = 7920	new error = 0.08367
i = 7964	new error = 0.08365
i = 7975	new error = 0.08355
i = 7992	new error = 0.08346
i = 8209	new error = 0.08342
i = 8252	new error = 0.08336
i = 8393	new error = 0.08336
i = 8395	new error = 0.08328
i = 8560	new error = 0.08323
i = 8671	new error = 0.08321
i = 8772	new error = 0.08313
i = 8816	new error = 0.08309
i = 8832	new error = 0.08309
i = 8941	new error = 0.08307
i = 9173	new error = 0.08307
i = 9212	new error = 0.08305
i = 9345	new error = 0.08302
i = 9460	new error = 0.08299
i = 9463	new error = 0.08290
i = 9484	new error = 0.08284
i = 9529	new error = 0.08283
i = 9602	new error = 0.08280
i = 9802	new error = 0.08279
i = 9813	new error = 0.08274
i = 9820	new error = 0.08270
i = 9908	new error = 0.08269
i = 9931	new error = 0.08265
i = 9951	new error = 0.08263
i = 9967	new error = 0.08248
i = 10177	new error = 0.08241
i = 10194	new error = 0.08238
i = 10215	new error = 0.08232
i = 10251	new error = 0.08230
i = 10402	new error = 0.08224
i = 10425	new error = 0.08224
i = 10461	new error = 0.08220
i = 10464	new error = 0.08220
i = 10562	new error = 0.08218
i = 10622	new error = 0.08215
i = 10637	new error = 0.08212
i = 10765	new error = 0.08205
i = 10843	new error = 0.08205
i = 10922	new error = 0.08201
i = 11022	new error = 0.08200
i = 11040	new error = 0.08188
i = 11160	new error = 0.08184
i = 11180	new error = 0.08184
i = 11296	new error = 0.08182
i = 11299	new error = 0.08180
i = 11342	new error = 0.08179
i = 11415	new error = 0.08176
i = 11583	new error = 0.08173
i = 11620	new error = 0.08172
i = 11643	new error = 0.08164
i = 11649	new error = 0.08161
i = 11821	new error = 0.08161
i = 11842	new error = 0.08150
i = 11930	new error = 0.08148
i = 11932	new error = 0.08143
i = 11947	new error = 0.08134
i = 12039	new error = 0.08128
i = 12154	new error = 0.08121
i = 12193	new error = 0.08120
i = 12231	new error = 0.08118
i = 12360	new error = 0.08106
i = 12380	new error = 0.08099
0.847
Loading the imdb reviews data
Data loaded.
Extracting features from the training dataset using a sparse vectorizer
Feature extraction technique is CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=5,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None).
done in 8.678247690200806s
n_samples: 12500, n_features: 19271 

Loading took 10.26s.

i = 19	new error = 0.08096
i = 77	new error = 0.08093
i = 82	new error = 0.08091
i = 88	new error = 0.08083
i = 114	new error = 0.08073
i = 168	new error = 0.08071
i = 177	new error = 0.08068
i = 191	new error = 0.08068
i = 329	new error = 0.08056
i = 369	new error = 0.08054
i = 483	new error = 0.08044
i = 558	new error = 0.08039
i = 609	new error = 0.08039
i = 727	new error = 0.08024
i = 731	new error = 0.08014
i = 978	new error = 0.08013
i = 1216	new error = 0.08012
i = 1231	new error = 0.08004
i = 1512	new error = 0.08004
i = 1540	new error = 0.08001
i = 1558	new error = 0.08001
i = 1590	new error = 0.07998
i = 1600	new error = 0.07990
i = 1648	new error = 0.07984
i = 1692	new error = 0.07979
i = 1705	new error = 0.07972
i = 1805	new error = 0.07971
i = 1915	new error = 0.07968
i = 1989	new error = 0.07961
i = 2046	new error = 0.07953
i = 2049	new error = 0.07949
i = 2149	new error = 0.07947
i = 2196	new error = 0.07947
i = 2197	new error = 0.07944
i = 2249	new error = 0.07937
i = 2361	new error = 0.07932
i = 2500	new error = 0.07930
i = 2598	new error = 0.07926
i = 2609	new error = 0.07925
i = 2654	new error = 0.07923
i = 2680	new error = 0.07922
i = 2777	new error = 0.07922
i = 2913	new error = 0.07921
i = 2963	new error = 0.07919
i = 2976	new error = 0.07917
i = 3026	new error = 0.07915
i = 3430	new error = 0.07911
i = 3510	new error = 0.07903
i = 3576	new error = 0.07894
i = 3786	new error = 0.07894
i = 3866	new error = 0.07891
i = 3905	new error = 0.07887
i = 3983	new error = 0.07886
i = 4028	new error = 0.07884
i = 4359	new error = 0.07883
i = 4635	new error = 0.07883
i = 4817	new error = 0.07879
i = 4821	new error = 0.07874
i = 4868	new error = 0.07869
i = 4894	new error = 0.07866
i = 4941	new error = 0.07864
i = 5187	new error = 0.07863
i = 5199	new error = 0.07861
i = 5230	new error = 0.07855
i = 5237	new error = 0.07847
i = 5301	new error = 0.07843
i = 5364	new error = 0.07840
i = 5500	new error = 0.07839
i = 5542	new error = 0.07838
i = 5661	new error = 0.07835
i = 5939	new error = 0.07834
i = 6055	new error = 0.07832
i = 6204	new error = 0.07829
i = 6286	new error = 0.07829
i = 6379	new error = 0.07821
i = 6525	new error = 0.07818
i = 6630	new error = 0.07818
i = 6652	new error = 0.07818
i = 6688	new error = 0.07816
i = 6718	new error = 0.07811
i = 6719	new error = 0.07796
i = 6748	new error = 0.07794
i = 6806	new error = 0.07788
i = 7195	new error = 0.07783
i = 7491	new error = 0.07783
i = 7622	new error = 0.07783
i = 7755	new error = 0.07782
i = 8084	new error = 0.07781
i = 8241	new error = 0.07779
i = 8298	new error = 0.07778
i = 8671	new error = 0.07777
i = 8760	new error = 0.07775
i = 8949	new error = 0.07775
i = 8963	new error = 0.07774
i = 9088	new error = 0.07773
i = 9089	new error = 0.07771
i = 9096	new error = 0.07766
i = 9319	new error = 0.07762
i = 9426	new error = 0.07761
i = 9508	new error = 0.07760
i = 9727	new error = 0.07758
i = 9756	new error = 0.07751
i = 9867	new error = 0.07748
i = 9913	new error = 0.07741
i = 9925	new error = 0.07735
i = 10233	new error = 0.07729
i = 10317	new error = 0.07727
i = 10373	new error = 0.07725
i = 10641	new error = 0.07725
i = 10795	new error = 0.07724
i = 10801	new error = 0.07723
i = 10868	new error = 0.07721
i = 10873	new error = 0.07720
i = 10891	new error = 0.07719
i = 10892	new error = 0.07712
i = 10980	new error = 0.07711
i = 10994	new error = 0.07710
i = 11126	new error = 0.07703
i = 11132	new error = 0.07699
i = 11175	new error = 0.07688
i = 11244	new error = 0.07681
i = 11460	new error = 0.07680
i = 11489	new error = 0.07678
i = 12137	new error = 0.07676
i = 12239	new error = 0.07674
i = 12326	new error = 0.07673
i = 12489	new error = 0.07670
0.84824
